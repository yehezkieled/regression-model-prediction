{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9V5ptJAIEbw"
   },
   "source": [
    "# FIT5197 2024 S1 Final Assessment\n",
    "\n",
    "**SPECIAL NOTE:** Please refer to the [assessment page](https://learning.monash.edu/mod/assign/view.php?id=2017255) for rules, general guidelines and marking rubrics of the assessment (the marking rubric for the kaggle competition part will be released near the deadline in the same page). Failure to comply with the provided information will result in a deduction of mark (e.g., late penalties) or breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkfAwCwbg7is"
   },
   "source": [
    "**YOUR NAME**: Yehezkiel Efraim Darmadi\n",
    "\n",
    "**STUDENT ID**: 34078215\n",
    "\n",
    "**KAGGLE NAME/ID** (See part 1, Question 5 or part 2, there are penalties if you don't enter it here!!!): Yehezkiel Efraim\n",
    "\n",
    "Please also enter your details in this [google form](https://forms.gle/isxqfrVnV7ddAj8y8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_u6Dyj1xehk"
   },
   "source": [
    "# Part 1 Regression (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "marked-instrument"
   },
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual. You need to build regression models to optimally predict the variable in the survey dataset called 'happiness' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```regression_train.csv``` and ```regression_test.csv```. Using these datasets, you hope to build a model that can predict happiness level using the other variables. ```regression_train.csv``` comes with the ground-truth target label (i.e. happiness level) whereas `regression_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict happiness. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. e.g., the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Gbtyt1g7iv"
   },
   "source": [
    "**PLEASE NOTE THAT THE USE OF LIBRARIES ARE PROHIBITED IN THESE QUESTIONS UNLESS STATED OTHERWISE, ANSWERS USING LIBRARIES WILL RECEIVE 0 MARKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wound-marriage"
   },
   "source": [
    "## Question 1 (NO LIBRARIES ALLOWED) (4 Mark)\n",
    "Please load the ```regression_train.csv``` and fit a [$\\textbf{multiple linear regression model}$](https://en.wikipedia.org/wiki/Linear_regression) with 'happiness' being the target variable. According to the summary table, which predictors do you think are possibly associated with the target variable (use the significance level of 0.01), and which are the **Top 5** strongest predictors? Please write an R script to automatically fetch and print this information.\n",
    "\n",
    "**NOTE**: Manually doing the above tasks will result in 0 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>p_values</th><th scope=col>features</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>11</th><td>6.160552e-75</td><td>income80k - 120k</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>2.033722e-47</td><td>income50k - 80k </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>1.902543e-39</td><td>income200k above</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>1.604206e-33</td><td>income20k - 50k </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>4.650658e-29</td><td>income15k - 20k </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & p\\_values & features\\\\\n",
       "  & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t11 & 6.160552e-75 & income80k - 120k\\\\\n",
       "\t10 & 2.033722e-47 & income50k - 80k \\\\\n",
       "\t8 & 1.902543e-39 & income200k above\\\\\n",
       "\t9 & 1.604206e-33 & income20k - 50k \\\\\n",
       "\t7 & 4.650658e-29 & income15k - 20k \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 2\n",
       "\n",
       "| <!--/--> | p_values &lt;dbl&gt; | features &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| 11 | 6.160552e-75 | income80k - 120k |\n",
       "| 10 | 2.033722e-47 | income50k - 80k  |\n",
       "| 8 | 1.902543e-39 | income200k above |\n",
       "| 9 | 1.604206e-33 | income20k - 50k  |\n",
       "| 7 | 4.650658e-29 | income15k - 20k  |\n",
       "\n"
      ],
      "text/plain": [
       "   p_values     features        \n",
       "11 6.160552e-75 income80k - 120k\n",
       "10 2.033722e-47 income50k - 80k \n",
       "8  1.902543e-39 income200k above\n",
       "9  1.604206e-33 income20k - 50k \n",
       "7  4.650658e-29 income15k - 20k "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "train_data <- read.csv(\"./regression_train.csv\")\n",
    "test_data <- read.csv(\"./regression_test.csv\")\n",
    "\n",
    "model <- lm(happiness ~ ., data = train_data)\n",
    "\n",
    "# p-values analysis\n",
    "p_values <- summary(model)$coefficients[, \"Pr(>|t|)\"]\n",
    "p_values_df <- data.frame(p_values)\n",
    "p_values_df$features <- rownames(p_values_df)\n",
    "rownames(p_values_df) <- NULL\n",
    "p_values_df <- p_values_df[order(p_values_df$p_values),]\n",
    "sig_level <- 0.01\n",
    "p_val_significant_df <- p_values_df[!is.na(p_values_df$p_values) & p_values_df$p_values <= sig_level, ]\n",
    "p_val_significant_df[1:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "royal-finland"
   },
   "source": [
    "## Question 2 (2 Mark)\n",
    "[**R squared**](https://en.wikipedia.org/wiki/Coefficient_of_determination) from the summary table reflects that the full model doesn't fit the training dataset well; thus, you try to quantify the error between the values of the ground-truth and those of the model prediction. You want to write a function to predict 'happiness' with the given dataset and calculate the [root mean squared error (rMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the model predictions and the ground truths. Please test this function on the full model and the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"This is training data RMSE: 6.67255660795814\"\n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "rmse_func <- function(model, df) {\n",
    "    predictions <- predict(model, df)\n",
    "    rmse <- sqrt(mean((predictions - df$happiness)^2))\n",
    "    return(rmse)\n",
    "}\n",
    "\n",
    "# calculate the RMSE\n",
    "full_model_train_rmse <- rmse_func(model, train_data)\n",
    "print(\n",
    "    paste0(\"This is training data RMSE: \", full_model_train_rmse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eight-newman"
   },
   "source": [
    "## Question 3 (2 Marks)\n",
    "You find the full model complicated and try to reduce the complexity by performing [bidirectional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) with [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "Calculate the **rMSE** of this new model with the function that you implemented previously. Is there anything you find unusual? Explain your findings in 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = happiness ~ income + alwaysStressed + alwaysHaveFun + \n",
       "    alwaysSerious + alwaysDepressed + iFindMostThingsAmusing + \n",
       "    iUsuallyHaveAGoodInfluenceOnEvents, data = train_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-33.365  -4.587  -0.030   5.203  18.888 \n",
       "\n",
       "Coefficients:\n",
       "                                   Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)                        -13.7027     0.6348 -21.587  < 2e-16 ***\n",
       "income10k - 15k                      6.9401     1.2123   5.725 1.82e-08 ***\n",
       "income120k - 150k                   12.4102     1.1859  10.465  < 2e-16 ***\n",
       "income150k - 200k                   12.1301     1.1885  10.206  < 2e-16 ***\n",
       "income15k - 20k                     14.0070     1.1820  11.851  < 2e-16 ***\n",
       "income200k above                    20.6292     1.3292  15.519  < 2e-16 ***\n",
       "income20k - 50k                     22.1535     1.4718  15.052  < 2e-16 ***\n",
       "income50k - 80k                     29.0212     1.6938  17.133  < 2e-16 ***\n",
       "income80k - 120k                    37.5672     1.5353  24.469  < 2e-16 ***\n",
       "alwaysStressed                      -1.7214     0.3071  -5.606 3.49e-08 ***\n",
       "alwaysHaveFun                        0.9188     0.2946   3.119 0.001923 ** \n",
       "alwaysSerious                       -0.7589     0.2981  -2.546 0.011203 *  \n",
       "alwaysDepressed                     -0.9029     0.3651  -2.473 0.013749 *  \n",
       "iFindMostThingsAmusing               0.9958     0.2996   3.324 0.000954 ***\n",
       "iUsuallyHaveAGoodInfluenceOnEvents   1.0721     0.3110   3.448 0.000614 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 7.443 on 485 degrees of freedom\n",
       "Multiple R-squared:  0.7206,\tAdjusted R-squared:  0.7126 \n",
       "F-statistic: 89.36 on 14 and 485 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The stepwise model with BIC RMSE on training data: 7.33030486894785\"\n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "stepwise_model <- step(model, trace = 0, k = log(nrow(train_data)), direction = \"both\")\n",
    "summary(stepwise_model)\n",
    "\n",
    "# calculate the RMSE\n",
    "step_model_train_rmse <- rmse_func(stepwise_model, train_data)\n",
    "print(\n",
    "    paste0(\"The stepwise model with BIC RMSE on training data: \", step_model_train_rmse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of the full model (6.67) is lower than that of the stepwise model (7.33), suggesting the full model fits the training data better. This difference indicates that while the stepwise model simplifies the model by reducing the number of predictors, it sacrifices some accuracy. The stepwise model’s higher RMSE might also reflect overfitting in the full model or the loss of important predictors during stepwise selection. It’s crucial to balance model complexity and accuracy, as simpler models are often more generalizable to new data despite having slightly higher RMSE on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "latter-translation"
   },
   "source": [
    "## Question 4 (2 Mark)\n",
    "Although stepwise regression has reduced the model complexity significantly, the model still contains a lot of variables that we want to remove. Therefore, you are interested in lightweight linear regression models with ONLY TWO predictors. Write a script to automatically find the best lightweight model which corresponds to the model with the least **rMSE** on the training dataset. Compare the **rMSE** of the best lightweight model with the **rMSE** of the full model - ```lm.fit``` - that you built previously. Give an explanation for these results based on consideration of the predictors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The top predictors are: income, and alwaysStressed\"\n",
      "[1] \"The lightweight model trin RMSE: 7.88541130213321\"\n",
      "[1] \"The full model train RMSE: 6.67255660795814\"\n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# get the selected features from the stepwise model\n",
    "selected_features <- colnames(train_data)[colnames(train_data) != \"happiness\"]\n",
    "\n",
    "# create combination of features\n",
    "predictor_pairs <- combn(selected_features, 2, simplify = FALSE)\n",
    "\n",
    "best_model <- NULL\n",
    "best_rmse <- Inf\n",
    "best_pair <- NULL\n",
    "\n",
    "for (pair in predictor_pairs) {\n",
    "    # create formula\n",
    "    formula <- as.formula(paste(\"happiness ~\", paste(paste0(\"`\", pair, \"`\"), collapse = \" + \")))\n",
    "    \n",
    "    # Fit the model\n",
    "    model <- lm(formula, data = train_data)\n",
    "\n",
    "    # Calculate RMSE for the current model\n",
    "    rmse <- rmse_func(model, train_data)\n",
    "\n",
    "    # Update the best model if the current model has a lower RMSE and also keep the best pair\n",
    "    if (rmse < best_rmse) {\n",
    "        best_model <- model\n",
    "        best_rmse <- rmse\n",
    "        best_pair <- pair\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\n",
    "    paste0(\"The top predictors are: \", paste(best_pair, collapse = \", and \"))\n",
    ")\n",
    "\n",
    "# calculate the lightweight_model's RMSE\n",
    "lightweight_model_train_rmse <- rmse_func(best_model, train_data)\n",
    "\n",
    "print(\n",
    "    paste0(\"The lightweight model trin RMSE: \", lightweight_model_train_rmse)\n",
    ")\n",
    "\n",
    "print(\n",
    "    paste0(\"The full model train RMSE: \", full_model_train_rmse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4P9QL6tg7ix"
   },
   "source": [
    "### ANSWER (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE for the best lightweight model (7.89) is higher than the RMSE for the full model (6.67). This means the full model, which includes more predictors, fits the training data better than the simpler lightweight model. The top predictors in the lightweight model are “income” and “alwaysStressed,” which significantly impact happiness. However, the full model’s additional predictors capture more complex relationships in the data, resulting in a more accurate fit. Thus, while the lightweight model is simpler, the full model provides a better understanding of the factors influencing happiness by considering more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preceding-chicago"
   },
   "source": [
    "## Question 5 (Libraries are allowed) (40 Marks)\n",
    "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limited to the aforementioned steps in this assignment. To simulate for a realistic modelling process, this question will be in the form of a [Kaggle competition](https://www.kaggle.com/t/ad8c96e412254c138cbec1d9d1c09734) among students to find out who has the best model.\n",
    "\n",
    "Thus, you **will be graded** by the **rMSE** performance of your model, the better your model, the higher your score. Additionally, you need to describe/document your thought process in this model building process, this is akin to showing your working properly for the mathematic sections. If you don't clearly document the reasonings behind the model you use, we will have to make some deductions on your scores.\n",
    "\n",
    "This is the [video tutorial](https://www.youtube.com/watch?v=rkXc25Uvyl4) on how to join any Kaggle competition. \n",
    "\n",
    "When you optimize your model's performance, you can use any supervised model that you know and feature selection might be a big help as well. [Check the non-exhaustive set of R functions relevant to this unit](https://learning.monash.edu/mod/resource/view.php?id=2017193) for ideas for different models to try.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "```install.packages(\"some package\", repos='http://cran.us.r-project.org')```\n",
    "\n",
    "```library(\"some package\")```\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction. Our suggestion is for you to use the standard ```R version 3.6.1```\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I used the kaggle notebook R version which is, 4.0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: caret\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Loading required package: tidyverse\n",
      "\n",
      "-- \u001b[1mAttaching packages\u001b[22m --------------------------------------- tidyverse 1.3.2 --\n",
      "\u001b[32mv\u001b[39m \u001b[34mtibble \u001b[39m 3.1.8      \u001b[32mv\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.10\n",
      "\u001b[32mv\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.1      \u001b[32mv\u001b[39m \u001b[34mstringr\u001b[39m 1.4.1 \n",
      "\u001b[32mv\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2      \u001b[32mv\u001b[39m \u001b[34mforcats\u001b[39m 0.5.2 \n",
      "\u001b[32mv\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4      \n",
      "-- \u001b[1mConflicts\u001b[22m ------------------------------------------ tidyverse_conflicts() --\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31mx\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mlift()\u001b[39m   masks \u001b[34mcaret\u001b[39m::lift()\n",
      "Loading required package: randomForest\n",
      "\n",
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "Loading required package: glmnet\n",
      "\n",
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: 'Matrix'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:tidyr':\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n",
      "Loaded glmnet 4.1-2\n",
      "\n",
      "Loading required package: nnet\n",
      "\n",
      "Loading required package: MLmetrics\n",
      "\n",
      "\n",
      "Attaching package: 'MLmetrics'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    MAE, RMSE\n",
      "\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Recall\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of required packages\n",
    "required_packages <- c(\"caret\", \"tidyverse\", \"randomForest\", \"glmnet\", \"nnet\", \"tibble\", \"MLmetrics\", \"ggplot2\")\n",
    "\n",
    "# Install any missing packages\n",
    "for (package in required_packages) {\n",
    "  if (!require(package, character.only = TRUE)) {\n",
    "    install.packages(package)\n",
    "    library(package, character.only = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load the libraries\n",
    "library(caret)\n",
    "library(tidyverse)\n",
    "library(randomForest)\n",
    "library(glmnet)\n",
    "library(nnet)\n",
    "library(tibble)\n",
    "library(MLmetrics)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will change every categorical features into factor. The reason is for this is:\n",
    "\n",
    "1. Like question one, the lm function will automatically one-hot encode everything.\n",
    "2. According to the above questions, the income will play big part in the model. Thus, one-hot encoded it makes the weightage bigger.\n",
    "3. Some of the categorical features like gender is suitable for the encoded type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the categorical columns\n",
    "categorical_columns <- sapply(train_data, function(col) is.factor(col) | is.character(col))\n",
    "\n",
    "# Get the names of categorical columns\n",
    "cat_colnames <- names(train_data)[categorical_columns]\n",
    "\n",
    "cat_data_tr <- train_data[, cat_colnames]  \n",
    "cat_data_ts <- test_data[, cat_colnames]\n",
    "\n",
    "# change every categorical data into factor\n",
    "for(i in colnames(cat_data_tr)){\n",
    "    cat_data_tr[[i]] <- as.factor(cat_data_tr[[i]])\n",
    "    cat_data_ts[[i]] <- as.factor(cat_data_ts[[i]])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the rest of the features, I will make everything above 0. The reason is that I want to do polynomial degree and for every even degree, it wont be making sense if the negative value is the same as the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rest of the colmns excluding the categorical columns and happiness\n",
    "non_categorical_columns <- setdiff(colnames(train_data %>% select(-happiness)), cat_colnames)\n",
    "non_cat_data_tr <- train_data[, non_categorical_columns]\n",
    "non_cat_data_ts <- test_data[, non_categorical_columns]\n",
    "\n",
    "# make function to convert every values into more than 0\n",
    "make_sequential <- function(data, cols) {\n",
    "  for (col in cols) {\n",
    "    # Get the column values\n",
    "    values <- data[[col]]\n",
    "    \n",
    "    # Shift values to ensure minimum value is 1\n",
    "    min_value <- min(values)\n",
    "    data[[col]] <- values - min_value + 1\n",
    "  }\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "# get the colnames of the non_cat_data_tr\n",
    "columns_to_sequential <- colnames(non_cat_data_tr)\n",
    "\n",
    "# transform each values\n",
    "non_cat_data_tr <- make_sequential(non_cat_data_tr, columns_to_sequential)\n",
    "non_cat_data_ts <- make_sequential(non_cat_data_ts, columns_to_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t500 obs. of  43 variables:\n",
      " $ gender                                                         : Factor w/ 3 levels \"Female\",\"Male\",..: 2 1 2 2 2 2 1 2 2 1 ...\n",
      " $ income                                                         : Factor w/ 9 levels \"0 - 10k\",\"10k - 15k\",..: 6 6 3 3 9 9 7 3 8 5 ...\n",
      " $ whatIsYourHeightExpressItAsANumberInMetresM                    : Factor w/ 10 levels \"140 - 150\",\"150 - 155\",..: 5 5 7 7 7 6 3 7 7 3 ...\n",
      " $ doYouFeelASenseOfPurposeAndMeaningInYourLife104                : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n",
      " $ howDoYouReconcileSpiritualBeliefsWithScientificOrRationalThinki: Factor w/ 3 levels \"They are separate\",..: 3 2 2 3 3 1 1 3 1 2 ...\n",
      " $ howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends      : Factor w/ 4 levels \"Always\",\"Never\",..: 4 3 4 3 4 4 4 2 4 2 ...\n",
      " $ doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded    : Factor w/ 4 levels \"No\",\"Yes, I have both\",..: 2 2 2 2 4 2 3 2 2 2 ...\n",
      " $ howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV: Factor w/ 5 levels \"At least once a month\",..: 4 3 1 4 5 4 5 1 2 1 ...\n",
      " $ doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer: Factor w/ 4 levels \"Always\",\"Never\",..: 2 1 4 1 2 1 4 2 1 2 ...\n",
      " $ doYouFeelASenseOfPurposeAndMeaningInYourLife105                : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 2 1 2 2 ...\n",
      " $ alwaysAnxious                                                  : num  1 1 2 3 4 2 3 5 1 2 ...\n",
      " $ alwaysStressed                                                 : num  2 1 2 4 4 2 2 4 1 2 ...\n",
      " $ alwaysAccountableAndResponsibleForYourActions                  : num  5 4 4 5 4 5 3 3 4 5 ...\n",
      " $ alwaysCalm                                                     : num  4 4 4 2 2 5 3 5 3 4 ...\n",
      " $ myBodyIsHypermobileAndLovesToMove                              : num  1 5 4 1 3 4 4 4 3 5 ...\n",
      " $ alwaysHaveFun                                                  : num  1 4 2 1 2 5 3 3 3 4 ...\n",
      " $ alwaysSerious                                                  : num  3 3 2 5 4 2 1 3 2 2 ...\n",
      " $ alwaysDepressed                                                : num  3 2 2 3 3 2 1 4 2 1 ...\n",
      " $ alwaysLoveAndCareForYourself                                   : num  5 3 4 2 2 5 1 2 1 5 ...\n",
      " $ extremelyGoodAbilityToSense                                    : num  5 4 4 3 4 4 4 5 3 5 ...\n",
      " $ alwaysHaveDigestiveProblems                                    : num  2 2 2 4 3 4 1 1 2 3 ...\n",
      " $ iAmIntenselyInterestedInOtherPeople                            : num  3 3 4 3 3 1 5 1 2 5 ...\n",
      " $ iRarelyWakeUpFeelingRested                                     : num  1 3 2 1 3 3 1 4 3 3 ...\n",
      " $ iFindMostThingsAmusing                                         : num  3 3 4 1 2 4 3 4 1 3 ...\n",
      " $ iAmAlwaysCommittedAndInvolved                                  : num  3 4 4 4 4 5 3 3 3 1 ...\n",
      " $ iDoNotThinkThatTheWorldIsAGoodPlace                            : num  3 1 2 5 4 2 1 4 3 2 ...\n",
      " $ iAmWellSatisfiedAboutEverythingInMyLife                        : num  3 2 3 1 1 5 1 2 2 4 ...\n",
      " $ iFindBeautyInSomeThings                                        : num  2 3 3 3 3 3 2 2 3 3 ...\n",
      " $ iAlwaysHaveACheerfulEffectOnOthers                             : num  3 3 4 1 1 4 3 1 1 5 ...\n",
      " $ iFeelThatIAmNotEspeciallyInControlOfMyLife                     : num  3 1 2 5 4 1 5 4 3 1 ...\n",
      " $ iUsuallyHaveAGoodInfluenceOnEvents                             : num  1 5 4 3 2 3 3 2 3 4 ...\n",
      " $ iDontHaveFunWithOtherPeople                                    : num  3 3 2 1 3 2 2 2 4 2 ...\n",
      " $ alwaysEngageInPreparingAndUsingYourSkillsAndTalentsInOrderToGai: num  4 5 4 1 2 4 1 1 3 3 ...\n",
      " $ alwaysMakingProgress                                           : num  5 1 4 1 3 5 3 3 3 5 ...\n",
      " $ extremelyGoodCommunicator                                      : num  4 3 4 3 3 4 3 3 1 4 ...\n",
      " $ iDontFeelParticularlyPleasedWithTheWayIAm                      : num  1 4 2 4 4 1 4 4 3 2 ...\n",
      " $ iFeelThatLifeIsVeryRewarding                                   : num  3 3 4 1 3 5 3 1 4 4 ...\n",
      " $ iHaveVeryWarmFeelingsTowardsAlmostEveryone                     : num  4 3 4 3 3 5 3 3 2 4 ...\n",
      " $ iAmNotParticularlyOptimisticAboutTheFuture                     : num  3 2 2 3 3 4 2 4 2 3 ...\n",
      " $ lifeIsGood                                                     : num  3 3 4 1 2 5 1 3 4 4 ...\n",
      " $ iLaughALot                                                     : num  3 3 3 1 1 4 4 3 2 5 ...\n",
      " $ iDontThinkILookAttractive                                      : num  1 2 2 4 4 3 3 3 3 3 ...\n",
      " $ happiness                                                      : num  16.679 5.553 -0.671 -6.009 14.143 ...\n"
     ]
    }
   ],
   "source": [
    "# combine the columns and add happiness\n",
    "final_data_tr <- cbind(cat_data_tr, non_cat_data_tr)\n",
    "final_data_tr$happiness <- train_data$happiness\n",
    "\n",
    "final_data_ts <- cbind(cat_data_ts, non_cat_data_ts)\n",
    "\n",
    "str(final_data_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will do feature selections using Random Forest. The reason is that the Random Forest eliminates values and also give the feature important scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        Feature\n",
      "income                                                                                                                   income\n",
      "alwaysStressed                                                                                                   alwaysStressed\n",
      "whatIsYourHeightExpressItAsANumberInMetresM                                         whatIsYourHeightExpressItAsANumberInMetresM\n",
      "iDontFeelParticularlyPleasedWithTheWayIAm                                             iDontFeelParticularlyPleasedWithTheWayIAm\n",
      "alwaysAnxious                                                                                                     alwaysAnxious\n",
      "alwaysCalm                                                                                                           alwaysCalm\n",
      "alwaysHaveFun                                                                                                     alwaysHaveFun\n",
      "howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\n",
      "iFindMostThingsAmusing                                                                                   iFindMostThingsAmusing\n",
      "alwaysDepressed                                                                                                 alwaysDepressed\n",
      "doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\n",
      "iUsuallyHaveAGoodInfluenceOnEvents                                                           iUsuallyHaveAGoodInfluenceOnEvents\n",
      "iAmWellSatisfiedAboutEverythingInMyLife                                                 iAmWellSatisfiedAboutEverythingInMyLife\n",
      "iLaughALot                                                                                                           iLaughALot\n",
      "lifeIsGood                                                                                                           lifeIsGood\n",
      "iAmIntenselyInterestedInOtherPeople                                                         iAmIntenselyInterestedInOtherPeople\n",
      "iAlwaysHaveACheerfulEffectOnOthers                                                           iAlwaysHaveACheerfulEffectOnOthers\n",
      "iFeelThatLifeIsVeryRewarding                                                                       iFeelThatLifeIsVeryRewarding\n",
      "howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends             howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\n",
      "iFeelThatIAmNotEspeciallyInControlOfMyLife                                           iFeelThatIAmNotEspeciallyInControlOfMyLife\n",
      "iDoNotThinkThatTheWorldIsAGoodPlace                                                         iDoNotThinkThatTheWorldIsAGoodPlace\n",
      "doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded         doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded\n",
      "alwaysSerious                                                                                                     alwaysSerious\n",
      "myBodyIsHypermobileAndLovesToMove                                                             myBodyIsHypermobileAndLovesToMove\n",
      "iDontHaveFunWithOtherPeople                                                                         iDontHaveFunWithOtherPeople\n",
      "iHaveVeryWarmFeelingsTowardsAlmostEveryone                                           iHaveVeryWarmFeelingsTowardsAlmostEveryone\n",
      "extremelyGoodAbilityToSense                                                                         extremelyGoodAbilityToSense\n",
      "iFindBeautyInSomeThings                                                                                 iFindBeautyInSomeThings\n",
      "iAmNotParticularlyOptimisticAboutTheFuture                                           iAmNotParticularlyOptimisticAboutTheFuture\n",
      "iDontThinkILookAttractive                                                                             iDontThinkILookAttractive\n",
      "                                                                Importance\n",
      "income                                                          44178.5553\n",
      "alwaysStressed                                                   5947.3374\n",
      "whatIsYourHeightExpressItAsANumberInMetresM                      4844.0585\n",
      "iDontFeelParticularlyPleasedWithTheWayIAm                        2279.3108\n",
      "alwaysAnxious                                                    1964.1777\n",
      "alwaysCalm                                                       1797.1594\n",
      "alwaysHaveFun                                                    1702.6001\n",
      "howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV  1656.2399\n",
      "iFindMostThingsAmusing                                           1577.4190\n",
      "alwaysDepressed                                                  1466.2387\n",
      "doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer  1462.7300\n",
      "iUsuallyHaveAGoodInfluenceOnEvents                               1148.5276\n",
      "iAmWellSatisfiedAboutEverythingInMyLife                          1087.5997\n",
      "iLaughALot                                                       1058.3139\n",
      "lifeIsGood                                                       1055.2031\n",
      "iAmIntenselyInterestedInOtherPeople                              1052.9990\n",
      "iAlwaysHaveACheerfulEffectOnOthers                               1016.4329\n",
      "iFeelThatLifeIsVeryRewarding                                      969.6117\n",
      "howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends         959.5005\n",
      "iFeelThatIAmNotEspeciallyInControlOfMyLife                        930.2709\n",
      "iDoNotThinkThatTheWorldIsAGoodPlace                               924.1132\n",
      "doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded       891.8883\n",
      "alwaysSerious                                                     885.0059\n",
      "myBodyIsHypermobileAndLovesToMove                                 884.9762\n",
      "iDontHaveFunWithOtherPeople                                       863.3716\n",
      "iHaveVeryWarmFeelingsTowardsAlmostEveryone                        852.4106\n",
      "extremelyGoodAbilityToSense                                       803.4302\n",
      "iFindBeautyInSomeThings                                           783.0309\n",
      "iAmNotParticularlyOptimisticAboutTheFuture                        779.8566\n",
      "iDontThinkILookAttractive                                         773.1146\n"
     ]
    }
   ],
   "source": [
    "# create rf model\n",
    "set.seed(42)\n",
    "rf_model <- randomForest(happiness ~ ., data = final_data_tr, importance = TRUE)\n",
    "\n",
    "# Extract feature importance scores\n",
    "importance_scores <- importance(rf_model)\n",
    "importance_df <- data.frame(Feature = rownames(importance_scores), Importance = importance_scores[, 'IncNodePurity'])\n",
    "\n",
    "# Select features with high importance\n",
    "importance_df <- importance_df %>% arrange(desc(Importance))\n",
    "selected_features_df <- importance_df[1:30,]\n",
    "\n",
    "# Print the selected features\n",
    "print(selected_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to avoid making polynomial features out of the categorical values. Thus, I am going to seperate the colnames for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the selected features \n",
    "selected_features <- selected_features_df$Feature\n",
    "\n",
    "# take the categorical features colnames\n",
    "feature_cat_df <- selected_features_df %>% filter(Feature %in% cat_colnames)\n",
    "feature_cat_names <-feature_cat_df$Feature\n",
    "\n",
    "# take the non categorical features colnames\n",
    "feature_not_cat_df <- selected_features_df %>% filter(!Feature %in% cat_colnames)\n",
    "feature_not_cat_names <- feature_not_cat_df$Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into the training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "# Subset the data to include only the selected features\n",
    "selected_train <- final_data_tr[, selected_features]\n",
    "selected_train$happiness <- final_data_tr$happiness\n",
    "\n",
    "# Create data partition\n",
    "trainIndex <- createDataPartition(selected_train$happiness, p = 0.8, list = FALSE, times = 1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "trainData <- selected_train[trainIndex, ]\n",
    "validationData <- selected_train[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will build parametric model because after trying the model with just basic lm and rpart function, I found that lm works better. Thus, I am sticking with parametric models.\n",
    "\n",
    "To get the parametric model perform well, there two things that we can do:\n",
    "\n",
    "1. Feature engineering\n",
    "2. Regularization\n",
    "\n",
    "Feature engineering such as polynomial degrees are useful to make the model learn better since the parametric models do not have lots of hyperparameter like non-parametric models.\n",
    "\n",
    "The regularization such as L1, L2, and elastic net are very useful to keep the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the polynomial degree from the non-categorical features according to the importance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"I(alwaysStressed^2)\"                            \n",
      " [2] \"I(iDontFeelParticularlyPleasedWithTheWayIAm^2)\" \n",
      " [3] \"I(alwaysAnxious^2)\"                             \n",
      " [4] \"I(alwaysCalm^2)\"                                \n",
      " [5] \"I(alwaysHaveFun^2)\"                             \n",
      " [6] \"I(iFindMostThingsAmusing^2)\"                    \n",
      " [7] \"I(alwaysDepressed^2)\"                           \n",
      " [8] \"I(iUsuallyHaveAGoodInfluenceOnEvents^2)\"        \n",
      " [9] \"I(iAmWellSatisfiedAboutEverythingInMyLife^2)\"   \n",
      "[10] \"I(iLaughALot^2)\"                                \n",
      "[11] \"I(lifeIsGood^2)\"                                \n",
      "[12] \"I(iAmIntenselyInterestedInOtherPeople^2)\"       \n",
      "[13] \"I(iAlwaysHaveACheerfulEffectOnOthers^2)\"        \n",
      "[14] \"I(iFeelThatLifeIsVeryRewarding^2)\"              \n",
      "[15] \"I(iFeelThatIAmNotEspeciallyInControlOfMyLife^2)\"\n",
      "[16] \"I(iDoNotThinkThatTheWorldIsAGoodPlace^2)\"       \n",
      "[17] \"I(alwaysSerious^2)\"                             \n",
      "[18] \"I(myBodyIsHypermobileAndLovesToMove^2)\"         \n",
      "[19] \"I(iDontHaveFunWithOtherPeople^2)\"               \n",
      "[20] \"I(iHaveVeryWarmFeelingsTowardsAlmostEveryone^2)\"\n",
      "[21] \"I(extremelyGoodAbilityToSense^2)\"               \n",
      "[22] \"I(iFindBeautyInSomeThings^2)\"                   \n",
      "[23] \"I(iAmNotParticularlyOptimisticAboutTheFuture^2)\"\n",
      "[24] \"I(iDontThinkILookAttractive^2)\"                 \n",
      "[25] \"I(alwaysStressed^3)\"                            \n",
      "[26] \"I(iDontFeelParticularlyPleasedWithTheWayIAm^3)\" \n",
      "[27] \"I(alwaysAnxious^3)\"                             \n",
      "[28] \"I(alwaysCalm^3)\"                                \n",
      "[29] \"I(alwaysHaveFun^3)\"                             \n",
      "[30] \"I(iFindMostThingsAmusing^3)\"                    \n",
      "[31] \"I(alwaysDepressed^3)\"                           \n",
      "[32] \"I(iUsuallyHaveAGoodInfluenceOnEvents^3)\"        \n",
      "[33] \"I(iAmWellSatisfiedAboutEverythingInMyLife^3)\"   \n",
      "[34] \"I(iLaughALot^3)\"                                \n",
      "[35] \"I(lifeIsGood^3)\"                                \n",
      "[36] \"I(iAmIntenselyInterestedInOtherPeople^3)\"       \n",
      "[37] \"I(iAlwaysHaveACheerfulEffectOnOthers^3)\"        \n",
      "[38] \"I(iFeelThatLifeIsVeryRewarding^3)\"              \n",
      "[39] \"I(iFeelThatIAmNotEspeciallyInControlOfMyLife^3)\"\n",
      "[40] \"I(iDoNotThinkThatTheWorldIsAGoodPlace^3)\"       \n",
      "[41] \"I(alwaysSerious^3)\"                             \n",
      "[42] \"I(myBodyIsHypermobileAndLovesToMove^3)\"         \n",
      "[43] \"I(iDontHaveFunWithOtherPeople^3)\"               \n",
      "[44] \"I(iHaveVeryWarmFeelingsTowardsAlmostEveryone^3)\"\n",
      "[45] \"I(extremelyGoodAbilityToSense^3)\"               \n",
      "[46] \"I(iFindBeautyInSomeThings^3)\"                   \n",
      "[47] \"I(iAmNotParticularlyOptimisticAboutTheFuture^3)\"\n",
      "[48] \"I(iDontThinkILookAttractive^3)\"                 \n",
      "[49] \"I(alwaysStressed^4)\"                            \n",
      "[50] \"I(iDontFeelParticularlyPleasedWithTheWayIAm^4)\" \n",
      "[51] \"I(alwaysAnxious^4)\"                             \n",
      "[52] \"I(alwaysCalm^4)\"                                \n",
      "[53] \"I(alwaysHaveFun^4)\"                             \n",
      "[54] \"I(iFindMostThingsAmusing^4)\"                    \n",
      "[55] \"I(alwaysDepressed^4)\"                           \n",
      "[56] \"I(iUsuallyHaveAGoodInfluenceOnEvents^4)\"        \n",
      "[57] \"I(iAmWellSatisfiedAboutEverythingInMyLife^4)\"   \n",
      "[58] \"I(iLaughALot^4)\"                                \n",
      "[59] \"I(lifeIsGood^4)\"                                \n",
      "[60] \"I(iAmIntenselyInterestedInOtherPeople^4)\"       \n",
      "[61] \"I(iAlwaysHaveACheerfulEffectOnOthers^4)\"        \n",
      "[62] \"I(iFeelThatLifeIsVeryRewarding^4)\"              \n",
      "[63] \"I(iFeelThatIAmNotEspeciallyInControlOfMyLife^4)\"\n",
      "[64] \"I(iDoNotThinkThatTheWorldIsAGoodPlace^4)\"       \n",
      "[65] \"I(alwaysSerious^4)\"                             \n",
      "[66] \"I(myBodyIsHypermobileAndLovesToMove^4)\"         \n",
      "[67] \"I(iDontHaveFunWithOtherPeople^4)\"               \n",
      "[68] \"I(iHaveVeryWarmFeelingsTowardsAlmostEveryone^4)\"\n",
      "[69] \"I(extremelyGoodAbilityToSense^4)\"               \n",
      "[70] \"I(iAmNotParticularlyOptimisticAboutTheFuture^4)\"\n",
      "[71] \"I(iDontThinkILookAttractive^4)\"                 \n"
     ]
    }
   ],
   "source": [
    "# empty variable to stores the polynomial features\n",
    "poly_degree_list <- c()\n",
    "\n",
    "# for looping from 2 to 5 (the maximum number of unique value in the features)\n",
    "for(i in 2:5){\n",
    "    for(j in feature_not_cat_names){\n",
    "        # if the i is less than the length of the unique values of the features then skip it\n",
    "        if(i < length(unique(final_data_tr[[j]]))){\n",
    "            # creating the formula\n",
    "            poly_feat <- paste0(\"I(\", j, \"^\", i, \")\")\n",
    "            poly_degree_list <- c(poly_degree_list, poly_feat)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(poly_degree_list)\n",
    "\n",
    "# create variable to store the whole features that are going to be tested\n",
    "features <- c(selected_features, poly_degree_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the RMSE for each of the number of features for the lm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty variables to store the RMSE value\n",
    "rmse_tr <- c()\n",
    "rmse_vl <- c()\n",
    "\n",
    "# for looping the number of features to be tested.\n",
    "for(i in 1:length(features)){\n",
    "    # create the formula\n",
    "    formula <- as.formula(paste(\"happiness ~\", paste(features[1:i], collapse = \" + \")))\n",
    "    # create the model\n",
    "    model <- lm(formula, data = trainData)\n",
    "    # store the RMSE scores\n",
    "    rmse_tr[i] <- rmse_func(model, trainData)\n",
    "    rmse_vl[i] <- rmse_func(model, validationData)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the RMSE training vs the validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP9NTU1oaGh8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\nAAlAJIAEIBJAAv4DP156c3MG3foAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine data into a data frame\n",
    "df <- data.frame(\n",
    "  Index = 1:length(rmse_tr),\n",
    "  RMSE_tr = rmse_tr,\n",
    "  RMSE_vl = rmse_vl\n",
    ")\n",
    "\n",
    "# Reshape data into long format using pivot_longer\n",
    "df_long <- df %>%\n",
    "  pivot_longer(cols = -Index, names_to = \"Variable\", values_to = \"Value\")\n",
    "\n",
    "# Create the plot using ggplot2\n",
    "ggplot(data = df_long, aes(x = Index, y = Value, color = Variable, group = Variable)) +\n",
    "  geom_line() +\n",
    "  geom_point() +\n",
    "  labs(x = \"Features\", y = \"RMSE\", title = \"RMSE Training vs Validation\") +\n",
    "  scale_color_manual(values = c(\"RMSE_tr\" = \"blue\", \"RMSE_vl\" = \"red\")) +\n",
    "  theme_minimal() +\n",
    "  theme(legend.title = element_blank())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying so many combinations, I chose to kind of overfit my model first and uses the elastic net regression to regularize the regression models.\n",
    "\n",
    "The number of features up until 78 works best with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final and testing model\n",
    "fin_formula <- as.formula(paste(\"happiness ~\", paste(features[1:78], collapse = \" + \")))\n",
    "test_formula <- as.formula(paste(\"~\", paste(features[1:78], collapse = \" + \")))\n",
    "\n",
    "# create model matrix for the train and validation dataset to be fitted in the glm function\n",
    "x_train <- model.matrix(fin_formula, trainData)[, -1]  # Remove intercept term\n",
    "y_train <- trainData$happiness\n",
    "x_val <- model.matrix(fin_formula, validationData)[, -1]\n",
    "y_val <- validationData$happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model on the trainData first to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:  cv.glmnet(x = x_train, y = y_train, alpha = 0.5) \n",
      "\n",
      "Measure: Mean-Squared Error \n",
      "\n",
      "    Lambda Index Measure    SE Nonzero\n",
      "min 0.2828    42   58.89 5.843      56\n",
      "1se 0.8637    30   64.32 6.373      38\n",
      "[1] \"Elastic Net Training RMSE: 6.65395019401319\"\n",
      "[1] \"Elastic Net Validation RMSE: 7.53351135040194\"\n",
      "[1] \"CV Elastic RMSE: 7.67394318248639\"\n"
     ]
    }
   ],
   "source": [
    "set.seed(42)\n",
    "# Elastic Net regression (alpha = 0.5)\n",
    "elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5)\n",
    "print(elastic_net_model)\n",
    "\n",
    "# predict the x_train and x_val\n",
    "elastic_pred_train <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_train)\n",
    "elastic_pred_val <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_val)\n",
    "\n",
    "# get the RMSE vallues\n",
    "elastic_rmse_train <- sqrt(mean((y_train - elastic_pred_train)^2))\n",
    "elastic_rmse_val <- sqrt(mean((y_val - elastic_pred_val)^2))\n",
    "print(paste(\"Elastic Net Training RMSE:\", elastic_rmse_train))\n",
    "print(paste(\"Elastic Net Validation RMSE:\", elastic_rmse_val))\n",
    "\n",
    "# Calculate the cross-validated RMSE\n",
    "cv_elastic_mse <- elastic_net_model$cvm[elastic_net_model$lambda == elastic_net_model$lambda.min]\n",
    "cv_elastic_rmse <- sqrt(cv_elastic_mse)\n",
    "print(paste(\"CV Elastic RMSE:\", cv_elastic_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model are not too overfitted.\n",
    "\n",
    "Let's build the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:  cv.glmnet(x = x_final, y = y_final, alpha = 0.5) \n",
      "\n",
      "Measure: Mean-Squared Error \n",
      "\n",
      "    Lambda Index Measure    SE Nonzero\n",
      "min 0.2808    42   57.64 4.476      44\n",
      "1se 0.7814    31   61.63 5.760      33\n",
      "[1] \"Fin CV Elastic RMSE: 7.59208876027628\"\n"
     ]
    }
   ],
   "source": [
    "# create model matrix for the train and validation dataset to be fitted in the glm function\n",
    "x_final <- model.matrix(fin_formula, final_data_tr)[, -1]\n",
    "y_final <- final_data_tr$happiness\n",
    "\n",
    "# Elastic Net regression (alpha = 0.5)\n",
    "set.seed(42)\n",
    "fin_elastic_net_model <- cv.glmnet(x_final, y_final, alpha = 0.5)\n",
    "print(fin_elastic_net_model)\n",
    "\n",
    "# Calculate the cross-validated RMSE\n",
    "cv_elastic_mse <- fin_elastic_net_model$cvm[fin_elastic_net_model$lambda == fin_elastic_net_model$lambda.min]\n",
    "cv_elastic_rmse <- sqrt(cv_elastic_mse)\n",
    "print(paste(\"Fin CV Elastic RMSE:\", cv_elastic_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model.\n",
    "save(fin_elastic_net_model, file = \"fin_elastic_net_model.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8x-H3d66g7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Build your final model here, use additional coding blocks if you need to\n",
    "load(\"fin_elastic_net_model.RData\")\n",
    "fin.mod <-fin_elastic_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j0SYx-zYg7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the testing data \n",
    "test <- model.matrix(test_formula, final_data_ts)[, -1]\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, s = fin_elastic_net_model$lambda.min, test)\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "submission_df <- data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label)\n",
    "colnames(submission_df) <- c(\"RowIndex\", \"Prediction\")\n",
    "write.csv(\n",
    "    submission_df,  \n",
    "    \"RegressionPredictLabel.csv\", \n",
    "    row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gross-disaster",
    "outputId": "ac8b1b5a-ad80-42f7-824a-61432e4d64cf",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "tryCatch(\n",
    "    {\n",
    "        source(\"../supplimentary.R\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        source(\"supplimentary.R\")\n",
    "    }\n",
    ")\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../regression_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"regression_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "RMSE.fin <- rmse(pred.label, truths$x)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_u6Dyj1xehk"
   },
   "source": [
    "# Part 2 Classification (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual, but this time we want to predict a categorical score for perfect mental health, rather than a continuous score. You need to build 5-class classification models to optimally predict the variable in the survey dataset called 'perfectMentalHealth' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```classification_train.csv``` and ```classification_test.csv```. Using these datasets, you hope to build a model that can predict 'perfectMentalHealth' using the other variables. ```classification_train.csv``` comes with the ground-truth target label (i.e. 'perfectMentalHealth' happiness classes) whereas `classification_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict 'perfectMentalHealth'. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. E.g. the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'\n",
    "\n",
    "This question will also be in the form of a [Kaggle competition](https://www.kaggle.com/t/968d3b346acb47779771f47785c39e62) among students to find out who has the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the train and test classification data.\n",
    "test_data <- read.csv(\"./classification_test.csv\")\n",
    "train_data <- read.csv(\"./classification_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Classification competition, I want to change everything to numeric. Because it performs well on the model and some of the categorical features like gender are going to be dropped by the feature selections. While income is a lot better as a numeric feature in this case.\n",
    "\n",
    "I will also create a lot of polynomial degrees, so it would make sense if I transform most of the features to be more than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t500 obs. of  43 variables:\n",
      " $ gender                                                         : num  1 1 2 2 1 1 2 2 1 1 ...\n",
      " $ income                                                         : num  2 3 1 9 3 1 8 2 4 1 ...\n",
      " $ whatIsYourHeightExpressItAsANumberInMetresM                    : num  3 5 4 4 6 4 7 4 3 4 ...\n",
      " $ doYouFeelASenseOfPurposeAndMeaningInYourLife104                : num  2 2 2 2 2 2 2 2 2 2 ...\n",
      " $ howDoYouReconcileSpiritualBeliefsWithScientificOrRationalThinki: num  2 3 2 2 1 2 2 2 2 1 ...\n",
      " $ howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends      : num  3 4 3 2 2 4 4 3 4 3 ...\n",
      " $ doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded    : num  2 2 2 1 2 2 2 3 2 2 ...\n",
      " $ howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV: num  3 3 3 1 2 2 2 2 1 5 ...\n",
      " $ doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer: num  4 4 2 2 1 3 4 4 4 4 ...\n",
      " $ doYouFeelASenseOfPurposeAndMeaningInYourLife105                : num  2 2 2 1 2 2 2 2 2 2 ...\n",
      " $ alwaysAnxious                                                  : num  2 3 5 3 1 2 2 1 3 1 ...\n",
      " $ alwaysStressed                                                 : num  1 4 4 3 4 2 3 3 3 1 ...\n",
      " $ alwaysAccountableAndResponsibleForYourActions                  : num  4 4 3 5 5 4 3 3 1 4 ...\n",
      " $ alwaysCalm                                                     : num  3 1 1 5 1 1 3 3 1 1 ...\n",
      " $ myBodyIsHypermobileAndLovesToMove                              : num  1 3 1 1 4 1 3 1 4 1 ...\n",
      " $ alwaysHaveFun                                                  : num  5 4 1 3 4 3 3 1 1 2 ...\n",
      " $ alwaysSerious                                                  : num  1 3 3 4 1 1 3 1 1 2 ...\n",
      " $ alwaysDepressed                                                : num  2 1 3 3 1 2 2 1 3 2 ...\n",
      " $ alwaysLoveAndCareForYourself                                   : num  5 5 2 5 4 5 3 3 3 4 ...\n",
      " $ extremelyGoodAbilityToSense                                    : num  4 4 3 4 5 4 3 4 1 3 ...\n",
      " $ alwaysHaveDigestiveProblems                                    : num  1 2 4 2 3 1 3 1 4 2 ...\n",
      " $ iAmIntenselyInterestedInOtherPeople                            : num  4 1 1 1 1 3 3 4 2 3 ...\n",
      " $ iRarelyWakeUpFeelingRested                                     : num  1 2 4 4 5 4 3 3 1 2 ...\n",
      " $ iFindMostThingsAmusing                                         : num  3 3 1 4 4 4 3 3 3 3 ...\n",
      " $ iAmAlwaysCommittedAndInvolved                                  : num  4 3 1 5 4 3 3 1 1 4 ...\n",
      " $ iDoNotThinkThatTheWorldIsAGoodPlace                            : num  3 3 2 4 2 1 3 3 4 4 ...\n",
      " $ iAmWellSatisfiedAboutEverythingInMyLife                        : num  1 3 3 4 3 3 3 3 3 2 ...\n",
      " $ iFindBeautyInSomeThings                                        : num  3 4 3 3 1 3 2 3 3 3 ...\n",
      " $ iAlwaysHaveACheerfulEffectOnOthers                             : num  3 3 3 3 3 3 3 3 2 1 ...\n",
      " $ iFeelThatIAmNotEspeciallyInControlOfMyLife                     : num  2 3 3 2 2 3 3 1 3 2 ...\n",
      " $ iUsuallyHaveAGoodInfluenceOnEvents                             : num  4 1 1 1 3 4 3 2 3 4 ...\n",
      " $ iDontHaveFunWithOtherPeople                                    : num  2 1 1 2 2 2 3 2 1 2 ...\n",
      " $ alwaysEngageInPreparingAndUsingYourSkillsAndTalentsInOrderToGai: num  5 4 1 4 5 4 3 3 3 4 ...\n",
      " $ alwaysMakingProgress                                           : num  5 4 3 5 3 4 3 4 3 3 ...\n",
      " $ extremelyGoodCommunicator                                      : num  4 3 4 5 3 3 3 3 3 5 ...\n",
      " $ iDontFeelParticularlyPleasedWithTheWayIAm                      : num  5 1 5 5 3 1 3 4 3 4 ...\n",
      " $ iFeelThatLifeIsVeryRewarding                                   : num  5 4 1 3 4 3 3 4 1 4 ...\n",
      " $ iHaveVeryWarmFeelingsTowardsAlmostEveryone                     : num  5 3 3 3 3 1 3 3 1 3 ...\n",
      " $ iAmNotParticularlyOptimisticAboutTheFuture                     : num  1 1 1 1 3 3 3 3 4 1 ...\n",
      " $ lifeIsGood                                                     : num  4 5 1 3 3 3 3 3 3 3 ...\n",
      " $ iLaughALot                                                     : num  3 3 4 3 4 3 3 4 5 4 ...\n",
      " $ iDontThinkILookAttractive                                      : num  1 2 3 1 1 4 3 1 3 3 ...\n",
      " $ perfectMentalHealth                                            : Factor w/ 5 levels \"-2\",\"-1\",\"0\",..: 5 4 3 1 4 1 3 3 2 3 ...\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns <- sapply(train_data, function(col) is.factor(col) | is.character(col))\n",
    "\n",
    "# Get the names of categorical columns\n",
    "cat_colnames <- names(train_data)[categorical_columns]\n",
    "cat_data_tr <- train_data[, cat_colnames]  \n",
    "cat_data_ts <- test_data[, cat_colnames]\n",
    "\n",
    "# Select the rest of the columns excluding categorical columns\n",
    "non_categorical_columns <- setdiff(colnames(train_data %>% select(-perfectMentalHealth)), cat_colnames)\n",
    "non_cat_data_tr <- train_data[, non_categorical_columns]\n",
    "non_cat_data_ts <- test_data[, non_categorical_columns]\n",
    "\n",
    "# make function to convert every values into more than 0\n",
    "make_sequential <- function(data, cols) {\n",
    "  for (col in cols) {\n",
    "    # Get the column values\n",
    "    values <- data[[col]]\n",
    "    \n",
    "    # Shift values to ensure minimum value is 1\n",
    "    min_value <- min(values)\n",
    "    data[[col]] <- values - min_value + 1\n",
    "  }\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "# get the colnames of the non_cat_data_tr\n",
    "columns_to_sequential <- colnames(non_cat_data_tr)\n",
    "\n",
    "# transform each values\n",
    "non_cat_data_tr <- make_sequential(non_cat_data_tr, columns_to_sequential)\n",
    "non_cat_data_ts <- make_sequential(non_cat_data_ts, columns_to_sequential)\n",
    "\n",
    "# combine the data into final_data_tr\n",
    "final_data_tr <- cbind(cat_data_tr, non_cat_data_tr)\n",
    "final_data_tr$perfectMentalHealth <- train_data$perfectMentalHealth\n",
    "\n",
    "final_data_ts <- cbind(cat_data_ts, non_cat_data_ts)\n",
    "\n",
    "\n",
    "# Convert categorical columns to factors and then to numeric\n",
    "final_data_tr[categorical_columns] <- lapply(final_data_tr[categorical_columns], function(x) as.numeric(as.factor(x)))\n",
    "final_data_ts[categorical_columns] <- lapply(final_data_ts[categorical_columns], function(x) as.numeric(as.factor(x)))\n",
    "\n",
    "# make the target variable as a factor since we want to do classification prediction model\n",
    "final_data_tr$perfectMentalHealth <- as.factor(train_data$perfectMentalHealth)\n",
    "final_data_tr <- final_data_tr %>% select(-perfectMentalHealth, perfectMentalHealth)\n",
    "str(final_data_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model selection that I am going to use is the Random Forest, because it eliminates features and also give the MeanDecreaseAccuracy and MeanDecreaseGini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          features\n",
      "1                                     alwaysLoveAndCareForYourself\n",
      "2                                                    alwaysHaveFun\n",
      "3                                                       alwaysCalm\n",
      "4                                                       lifeIsGood\n",
      "5                                                   alwaysStressed\n",
      "6  alwaysEngageInPreparingAndUsingYourSkillsAndTalentsInOrderToGai\n",
      "7                          iAmWellSatisfiedAboutEverythingInMyLife\n",
      "8                              iDoNotThinkThatTheWorldIsAGoodPlace\n",
      "9                       iFeelThatIAmNotEspeciallyInControlOfMyLife\n",
      "10                                     extremelyGoodAbilityToSense\n",
      "11                                    iFeelThatLifeIsVeryRewarding\n",
      "12                                          iFindMostThingsAmusing\n",
      "13                                                          income\n",
      "14                      iHaveVeryWarmFeelingsTowardsAlmostEveryone\n",
      "15                                                   alwaysAnxious\n",
      "16                 doYouFeelASenseOfPurposeAndMeaningInYourLife105\n",
      "17                                                 alwaysDepressed\n",
      "18                                      iRarelyWakeUpFeelingRested\n",
      "19       howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\n",
      "20                       iDontFeelParticularlyPleasedWithTheWayIAm\n",
      "21                              iUsuallyHaveAGoodInfluenceOnEvents\n",
      "   MeanDecreaseAccuracy MeanDecreaseGini\n",
      "1             12.459198        16.266666\n",
      "2             11.956626        14.048295\n",
      "3             10.514191        14.088636\n",
      "4              9.219142        11.222465\n",
      "5              7.404869        12.487298\n",
      "6              5.022587        11.204215\n",
      "7              4.400194        10.190582\n",
      "8              4.170826        10.003101\n",
      "9              4.102139        11.087689\n",
      "10             4.007639         9.146962\n",
      "11             3.948584         9.377747\n",
      "12             3.671771         8.965866\n",
      "13             3.581918        12.756288\n",
      "14             3.579256        10.958011\n",
      "15             3.235090         9.261023\n",
      "16             2.588523         3.418119\n",
      "17             2.530895         7.571307\n",
      "18             2.245134        11.049474\n",
      "19             2.221891         8.779042\n",
      "20             2.127129         9.858243\n",
      "21             1.939560         8.088831\n"
     ]
    }
   ],
   "source": [
    "# create rf model\n",
    "set.seed(42)\n",
    "rf_model <- randomForest(perfectMentalHealth ~ ., data = final_data_tr, importance = TRUE)\n",
    "\n",
    "# Extract feature importance scores\n",
    "importance_scores <- importance(rf_model)\n",
    "importance_df <- data.frame(importance_scores) %>% \n",
    "    select(MeanDecreaseAccuracy, MeanDecreaseGini) %>%\n",
    "    rownames_to_column(var = \"features\") %>%\n",
    "    arrange(desc(MeanDecreaseAccuracy), desc(MeanDecreaseGini))\n",
    "\n",
    "# Select features with high importance\n",
    "selected_features_df <- importance_df[1:21,]\n",
    "\n",
    "# Print the selected features\n",
    "print(selected_features_df)\n",
    "selected_features <- selected_features_df$features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be choosing parametric model for the classification, it is because I already tried to see the score for the macro F1 score for the basic DT and basic multinomial, and the parametric model F1 score consistently perform well.\n",
    "\n",
    "Next, let's split the data into train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(150)\n",
    "# Subset the data to include only the selected features\n",
    "selected_train <- final_data_tr[, selected_features]\n",
    "selected_train$perfectMentalHealth <- final_data_tr$perfectMentalHealth\n",
    "\n",
    "# Create data partition\n",
    "trainIndex <- createDataPartition(selected_train$perfectMentalHealth, p = 0.8, list = FALSE, times = 1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "trainData <- selected_train[trainIndex, ]\n",
    "validationData <- selected_train[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to calculate the macro F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "macro_f1_score <- function(model, df) {\n",
    "    # Generate predictions\n",
    "    predictions <- predict(model, df)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confusion_matrix <- confusionMatrix(predictions, df$perfectMentalHealth)\n",
    "    \n",
    "    # Extract F1 scores by class\n",
    "    f1_scores <- confusion_matrix$byClass[, \"F1\"]\n",
    "    \n",
    "    # Calculate the macro F1 score\n",
    "    macro_f1_score <- mean(f1_scores, na.rm = TRUE)\n",
    "    \n",
    "    # Print the macro F1 score\n",
    "    return(macro_f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty variable to stores the polynomial features\n",
    "poly_degree_list <- c()\n",
    "\n",
    "for(i in 2:10){\n",
    "    for(j in selected_features){\n",
    "        if(i < length(unique(final_data_tr[[j]]))){\n",
    "            poly_feat <- paste0(\"I(\", j, \"^\", i, \")\")\n",
    "            poly_degree_list <- c(poly_degree_list, poly_feat)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create the variable that store the features\n",
    "features <- c(selected_features, poly_degree_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the F1 score for every possible number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store variables to store the F1 score\n",
    "macro_f1_tr <- c()\n",
    "macro_f1_vl <- c()\n",
    "\n",
    "# for looping the number of features\n",
    "for(i in 1:length(features)){\n",
    "    # create the formula\n",
    "    formula <- as.formula(paste(\"perfectMentalHealth ~\", paste(features[1:i], collapse = \" + \")))\n",
    "    # create the model\n",
    "    model <- multinom(formula, data = trainData, trace = FALSE)\n",
    "    # store the F1 score\n",
    "    macro_f1_tr[i] <- macro_f1_score(model, trainData)\n",
    "    macro_f1_vl[i] <- macro_f1_score(model, validationData)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAPFBMVEUAAAAAAP8A/wBNTU1o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\nIIJEURVEkCiqgv4HyqxnzgIXlaAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the difference between the training and validation F1 scores\n",
    "macro_f1_diff <- macro_f1_tr - macro_f1_vl\n",
    "\n",
    "# Combine data into a data frame\n",
    "df <- data.frame(\n",
    "  Index = 1:length(macro_f1_tr),\n",
    "  Macro_F1_Train = macro_f1_tr,\n",
    "  Macro_F1_Val = macro_f1_vl,\n",
    "  Macro_F1_Diff = macro_f1_diff\n",
    ")\n",
    "\n",
    "# Reshape data into long format using pivot_longer\n",
    "df_long <- df %>%\n",
    "  pivot_longer(cols = -Index, names_to = \"Variable\", values_to = \"Value\")\n",
    "\n",
    "# Create the plot using ggplot2\n",
    "ggplot(data = df_long, aes(x = Index, y = Value, color = Variable, group = Variable)) +\n",
    "  geom_line() +\n",
    "  geom_point() +\n",
    "  labs(x = \"Features\", y = \"Macro F1 score\", title = \"Macro F1 score Training vs Validation vs Difference\") +\n",
    "  scale_color_manual(values = c(\"Macro_F1_Train\" = \"blue\", \"Macro_F1_Val\" = \"red\", \"Macro_F1_Diff\" = \"green\")) +\n",
    "  theme_minimal() +\n",
    "  theme(legend.title = element_blank())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying so many values for the number of features, 55 features work the best for the model.\n",
    "\n",
    "Let's assess it on the trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  285 (224 variable)\n",
      "initial  value 646.994041 \n",
      "iter  10 value 544.716847\n",
      "iter  20 value 523.368412\n",
      "iter  30 value 513.093137\n",
      "iter  40 value 506.630829\n",
      "iter  50 value 501.216539\n",
      "iter  60 value 497.575898\n",
      "iter  70 value 486.612942\n",
      "iter  80 value 475.514426\n",
      "iter  90 value 455.675880\n",
      "iter 100 value 417.081255\n",
      "final  value 417.081255 \n",
      "stopped after 100 iterations\n",
      "[1] \"The F1 score for the trainData is 0.575702488085092\"\n",
      "[1] \"The F1 score for the validationData is 0.370701722133259\"\n"
     ]
    }
   ],
   "source": [
    "# create the final formula\n",
    "fin_formula <- as.formula(\n",
    "    paste(\n",
    "        \"perfectMentalHealth ~\", \n",
    "        paste(features[1:55], collapse = \" + \")\n",
    "    )\n",
    ")\n",
    "\n",
    "# create the model\n",
    "model <- multinom(fin_formula, data = trainData)\n",
    "\n",
    "# calculate the F1 score\n",
    "print(\n",
    "    paste(\"The F1 score for the trainData is\", macro_f1_score(model, trainData))\n",
    ")\n",
    "print(\n",
    "    paste(\"The F1 score for the validationData is\", macro_f1_score(model, validationData))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 scores suggest that the model is good but a bit overfit. \n",
    "\n",
    "Now let's create the final model using the cross validation and also the decay value to make the model not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"-2\" \"-1\" \"0\"  \"1\"  \"2\" \n",
      "[1] \"X.2\" \"X.1\" \"X0\"  \"X1\"  \"X2\" \n"
     ]
    }
   ],
   "source": [
    "# create new vairable to be transformed and put into the final model\n",
    "cv_final_data_tr <- final_data_tr\n",
    "\n",
    "# Print the modified levels to verify\n",
    "print(levels(cv_final_data_tr$perfectMentalHealth))\n",
    "\n",
    "# Convert levels to valid R variable names\n",
    "levels(cv_final_data_tr$perfectMentalHealth) <- make.names(levels(cv_final_data_tr$perfectMentalHealth))\n",
    "\n",
    "# Print the modified levels to verify\n",
    "print(levels(cv_final_data_tr$perfectMentalHealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  decay\n",
      "4  0.05\n",
      "Penalized Multinomial Regression \n",
      "\n",
      "500 samples\n",
      " 21 predictor\n",
      "  5 classes: 'X.2', 'X.1', 'X0', 'X1', 'X2' \n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (5 fold) \n",
      "Summary of sample sizes: 400, 400, 399, 399, 402 \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  decay  logLoss   AUC        prAUC      Accuracy   Kappa      Mean_F1  \n",
      "  0.02   1.614543  0.6910394  0.3221214  0.3378185  0.1497652  0.3326685\n",
      "  0.03   1.610869  0.6912849  0.3214725  0.3378185  0.1497652  0.3326685\n",
      "  0.04   1.607581  0.6916060  0.3213252  0.3378185  0.1494146  0.3326984\n",
      "  0.05   1.607151  0.6921548  0.3217376  0.3458185  0.1602466  0.3400810\n",
      "  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n",
      "  0.3439135         0.8287998         0.3290762            0.8295531          \n",
      "  0.3439135         0.8287998         0.3290762            0.8295531          \n",
      "  0.3439135         0.8286798         0.3294045            0.8294216          \n",
      "  0.3515392         0.8308788         0.3372865            0.8316471          \n",
      "  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n",
      "  0.3290762       0.3439135    0.06756371           0.5863567             \n",
      "  0.3290762       0.3439135    0.06756371           0.5863567             \n",
      "  0.3294045       0.3439135    0.06756371           0.5862967             \n",
      "  0.3372865       0.3515392    0.06916371           0.5912090             \n",
      "\n",
      "Accuracy was used to select the optimal model using the largest value.\n",
      "The final value used for the model was decay = 0.05.\n",
      "[1] \"The F1 Score is 0.565591326216504\"\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for cross-validation\n",
    "tune_grid <- expand.grid(\n",
    "  decay = c(0.02, 0.03, 0.04, 0.05)\n",
    ")\n",
    "\n",
    "# Define the control function for train with cross-validation\n",
    "train_control <- trainControl(\n",
    "  method = \"cv\",    # Cross-validation\n",
    "  number = 5,       # Number of folds\n",
    "  summaryFunction = multiClassSummary,\n",
    "  classProbs = TRUE\n",
    ")\n",
    "\n",
    "set.seed(150)\n",
    "# Train the model with cross-validation\n",
    "cv_model <- train(\n",
    "  fin_formula,\n",
    "  data = cv_final_data_tr,\n",
    "  method = \"multinom\",\n",
    "  trControl = train_control,\n",
    "  tuneGrid = tune_grid,\n",
    "    trace = FALSE\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters and results\n",
    "print(cv_model$bestTune)\n",
    "print(cv_model)\n",
    "\n",
    "\n",
    "# Define the reference mapping\n",
    "reference_levels <- c(\"X.2\", \"X.1\", \"X0\", \"X1\", \"X2\")\n",
    "numeric_values <- c(-2, -1, 0, 1, 2)\n",
    "# Create a named vector for mapping\n",
    "mapping <- setNames(numeric_values, reference_levels)\n",
    "\n",
    "# Generate predictions\n",
    "predictions <- predict(cv_model, cv_final_data_tr)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_matrix <- confusionMatrix(predictions, cv_final_data_tr$perfectMentalHealth)\n",
    "\n",
    "# Extract F1 scores by class\n",
    "f1_scores <- confusion_matrix$byClass[, \"F1\"]\n",
    "\n",
    "# Calculate the macro F1 score\n",
    "macro_f1score <- mean(f1_scores, na.rm = TRUE)\n",
    "\n",
    "# Print the macro F1 score\n",
    "print(\n",
    "    paste(\"The F1 Score is\", macro_f1score)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "save(cv_model, file = \"cv_multinom_model.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8x-H3d66g7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "load(\"cv_multinom_model.RData\")\n",
    "\n",
    "# Build your final model here, use additional coding blocks if you need to\n",
    "fin.mod <- cv_model\n",
    "\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, final_data_ts)\n",
    "pred.label <- as.numeric(as.character(mapping[as.character(pred.label)]))\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "write.csv(\n",
    "    data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "    \"ClassificationPredictLabel.csv\", \n",
    "    row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../classification_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"classification_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "f1_score <- F1_Score(truths$x, pred.label)\n",
    "cat(paste(\"f1_score is\", f1_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5197_Assignment_Marking_Guidelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
